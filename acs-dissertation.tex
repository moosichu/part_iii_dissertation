%%
%% ACS project dissertation template.
%%
%% Currently designed for printing two-sided, but if you prefer to
%% print single-sided just remove ",twoside,openright" from the
%% \documentclass[] line below.
%%
%%
%%   SMH, May 2010.


\documentclass[a4paper,12pt,twoside,openright]{report}


%%
%% EDIT THE BELOW TO CUSTOMIZE
%%

\def\authorname{Tom M. Read Cutting\xspace}
\def\authorcollege{Downing College\xspace}
\def\authoremail{tr395@cam.ac.uk}
\def\dissertationtitle{Heterogeneous type checking in multi-language CPU-GPU systems}
\def\wordcount{TODO}


\usepackage{array,color,epsfig,float,inconsolata,graphicx,hyperref,listings,parskip,setspace,tabularx,tabu,textcomp,xspace}
\graphicspath{ {images/} }

\newfloat{lstfloat}{htbp}{lop}
\floatname{lstfloat}{Listing}
\def\lstfloatautorefname{Listing} % needed for hyperref/auroref

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

%% START OF DOCUMENT
\begin{document}


%% FRONTMATTER (TITLE PAGE, DECLARATION, ABSTRACT, ETC)
\pagestyle{empty}
\singlespacing
\input{titlepage}
\onehalfspacing
\input{declaration}
\singlespacing
\input{abstract}

\pagenumbering{roman}
\setcounter{page}{0}
\pagestyle{plain}
\tableofcontents
% \listoffigures
% \listoftables

\onehalfspacing

%% START OF MAIN TEXT

\chapter{Introduction}
\pagenumbering{arabic}
\setcounter{page}{1}

This chapter provides the motivation for the work this paper presents:
Outlining the problem, why it is important and why it has not yet been solved.
This is followed by a brief description of the two solutions presented in this
paper: a cross-module annotation checker and two languages with a unified
cross-module type checker. Finally, an overview of the entire document is
provided.

\section{Terminology}

Useful terms and domain-specific jargon are briefly explained below. Although
the concepts here are explored in depth in Chapter
\ref{chp:technical_background}, this serves as a quick reference. Finally, the
end of this section quickly clarifies common points of confusion.

\begin{itemize}

    \item \textbf{GPU:} This is a \textit{Graphical Processing Unit}, and is a
    piece of hardware that performs computations using a massively parallel
    architecture (see Section \ref{sec:gpu_hardware}. GPUs are always
    controlled by the CPU through an API, which the CPU uses to manage GPU
    resources and load code onto the GPU. The RAM a GPU directly accesses is
    called VRAM. Although computing architectures do exist where the CPU and
    GPU share access to a single pool of memory, in most architectures data
    must be explicitly transferred from the CPU's RAM to the GPU's VRAM through
    a bus.

    \item \textbf{Shader/Kernel:} A \textit{shader} is \textit{any} computer
    program written for the GPU. These are named \textit{shaders} because they
    were originally used for shading computer graphics images, but the term now
    applies to all GPU programs including those which are not related to
    graphics at all \footnote{The original specification for the first OpenGL
    shading language discusses how this decision was made \cite{GLSL_1_10}.} A
    \textit{kernel} is a name used for a computer program that runs on any
    non-CPU \textit{accelerator}, and is a term that is widely used for general
    purpose heterogeneous computing.

    \item \textbf{Shading langauge:} These are programming languages that are
    used to write shaders. These are usually distinct languages from those used
    to write programs for the CPU. In the world of high performance computing
    these can also be called \textbf{kernel languages}.

    \item \textbf{Host language/code:} Programming languages and the resulting
    code which is written for and runs on the CPU. \textbf{Host} will also be
    used as a synonym for the CPU.

    \item \textbf{3D graphics library:} These are libraries for the CPU,
    commonly written in C. They provide helpful functions for graphics
    programmers and additionally provide appropriate APIs for interfacing with
    the GPU in order to render 3D graphics. This includes function and API
    calls to load and run shaders on the GPU.

    \item \textbf{Compute platform:} These are libraries and ecosystems similar
    to 3D graphics libraries. Although they serve the same purpose as graphics
    libraries of allowing programmers to interface with and load programs onto
    the GPU, they are designed with a focus on general purpose GPU
    computations. Additionally, some compute platforms are designed for
    hardware beyond GPUs.

    \item \textbf{OpenGL/Direct3D:} These are 3D graphics libraries. The first
    is an open standard maintained by the Khronos Group, and the
    latter is Microsoft's proprietary 3D graphics API supported by Windows
    platforms \cite{OpenGL} \cite{Direct3D}.

    \item \textbf{GLSL/HLSL:} These are \textit{shading languages}, that is,
    programming languages that are used to write shaders. GLSL is the language
    OpenGL uses whilst HLSL is the equivalent found in Direct3D. Traditionally,
    these languages are packaged as source-code with software that has been
    written to use them (such as a game). GPU vendors are responsible for
    writing drivers which compile source-code down to GPU machine-code at
    run-time.

    \item \textbf{OpenCL/CUDA:} These are \textit{compute platforms}. OpenCL is
    an open standard for the ``parallel programming of heterogeneous systems'',
    whilst CUDA is a similar, proprietary system that is only compatible with
    NVIDIA GPUs \cite{OpenCL} \cite{CUDA}. \textit{OpenCL C} and \textit{CUDA}
    are also the names of the shader languages these platforms use.

    \item \textbf{Vulkan:} This is 3D graphics library \textit{and} compute
    platform that is a successor to both OpenGL and OpenCL \cite{Vulkan}. It
    differs from OpenGL and OpenCL in creating a unified approach for
    interacting with GPUs at a much lower level than those libraries. Although
    it has seen uptake in the gaming industry, it has so far had little success
    in fields where OpenCL or CUDA have traditionally been used \cite{TODO}
    \cite{TODO}. \textbf{Metal} is a similar proprietary library by Apple
    \cite{Metal}.

    \item \textbf{SPIR-V}: This is an intermediate language successor to GLSL
    that is supported by OpenCL, OpenGL and Vulkan. The idea is that graphics
    drivers should only have to support interpreting this language instead of
    parsing a high-level language such as GLSL or OpenCL C. The Khronos Group
    hopes that this will spur the development of many varieties of GPU-targeted
    languages, similar to how there are many languages which compile to CPU
    machine-code \cite{SPIRV}. OpenCL C++ is such a language that has been
    successfully developed as a result \cite{OpenCLCPPWhitePaper}.

    \item \textbf{Framerate:} The rate at which a piece of software can render
    a new frame in real-time. Higher is better.

    \item \textbf{FPS:} Frames-per-second, a unit of measurement used to
    quantify the framerate of some software. Due to the refresh-rates of most
    consumer displays, 30FPS and 60FPS are common framerate targets.

\end{itemize}


Because heterogeneous programming is such a young field there are points of
confusion. For example, many papers creating custom interfaces or languages
claim to create ``OpenCL Code'' in their backend \cite{JITGPU} \cite{Lime2012}.
However, \textit{there is no such thing}. OpenCL is an API, and has
traditionally defined a C-like language called \textit{OpenCL C} as the
standard for writing compute kernels. Host code can then load it onto
heterogeneous devices using the OpenCL API. Many papers will output code in
this langauge for GPUs as a backend, as traditionally this has been the only
option available (aside from proprietary backends such as CUDA for NVIDIA
GPUs). In a recent development, SPIR-V can now be targeted as a back-end by
compilers instead of \textit{OpenCL C}. However, SPIR-V has yet to be targeted
by many new front-ends apart from \textit{OpenCL C++}
\cite{OpenCLCPPWhitePaper}.

\section{Motivation}

% This section introduces how a slow-down in ``Moore's Law'' has led to GPUs
% becoming increasingly relevant as more industries have turned to using them due
% to the the increased floating point computing power they provide relative to
% CPUs. However, I then explain how despite sizeable number of developers using
% GPUs, standards and standard practices are few and far between -- with a brief
% exploration of how legacy APIs, proprietary technologies have led to an
% unfortunate amount of fragmentation and steep learning curves for using GPUs.
% This is an important problem to work on due to the benefits solutions could
% provide to the wide number of developers programming for GPUs.

NOTE: THIS SECTION HAS NOT BEEN MODIFIED (MUCH) SINCE LAST READ.

As ``Moore's Law'' is arguably slowing down, Graphical Processing Units (GPUs)
have provided significant gains in floating-point computing power relative to
CPUs due to their highly parallel nature \cite{MooreLawSlowdown}
\cite{CPUGPUOverTime}. This has lead to a growth in the use of GPUs for general
purpose computations (known as GPGPU). Whilst GPUs were originally designed for
rendering the graphics of games using a fixed-function pipeline, changes in
their design over time have made them increasingly programmable. These design
changes were driven by the desire of game developers to have increasingly
realistic, complex and diverse graphics in their games. Furthermore, this has
allowed GPUs to be used in a broadening domain of applications, including
machine learning, scientific computing, and bitcoin mining \cite{GPUCrypto}
\cite{GPUScientificComputing} \cite{GPUAI}. Section \ref{sec:history_gpu}
provides more details on this.

However despite this growth, the toolchains used to interact with GPUs still
suffer from many of the problems they did when originally used by graphics
programmers in the early-to-mid 2000s.

\begin{itemize}

    \item The redefinition of data structures between languages.

\end{itemize}

A large root-cause of these problems is the fact that the programs written for
GPUs (known as \textit{shaders}) have to be developed in a separate
environment. Therefore although data is shared between the CPU and GPU via a
bus, the programmer has to manually ensure that the data structures used to
encode that data is consistent between the programs written for the GPU and
CPU.

\begin{lstfloat}
\begin{lstlisting}[language=C]
// C# struct definition
public struct WaveParticle
{
    public Vector2 origin;
    public Vector2 velocity;
    public float amplitude;
    public float dispersionAngle;
    public int startingFrame;
}
\end{lstlisting}
\begin{lstlisting}[language=C]
//HLSL struct definition
struct WaveParticle {
    float2 origin;
    float2 velocity;
    float amplitude;
    float dispersionAngle;
    int startingFrame;
};
\end{lstlisting}
\caption{The same data structure defined separately in C$^\sharp$ and HLSL. It
is worth noting that this is much less of an issue when using C as a host
language. This is because most shading languages are based on C, so they are
able to share header files.}
\label{lst:c_sharp_hlsl_struct_comparison}
\end{lstfloat}

Listing \ref{lst:c_sharp_hlsl_struct_comparison} demonstrates a simple example
of this kind of issue, extracted from a simple GPU-accelerated wave simulator
\cite{WaveParticlesGPU}. In this example, units of distortion on the surface of
a liquid are represented using a data structure known as a \textit{Wave
Particle} \cite{WaveParticlesOriginalPaper}. The CPU handles in-world physics
which result in these particles being generated. However, the GPU is used to
simulate the particles themselves and additionally calculates the shape of the
resulting surface they represent before rendering the results as shown in
Figure \ref{fig:waveparticles_example}. In this case, C$^\sharp$ was the
language used to write code for the CPU, and HLSL was the language used to
write shaders for the GPU. Therefore, even though both languages require the
same data structure, it has to be defined in both languages. If these
definitions do not agree with each other, there are no in-built mechanisms to
catch the errors this causes, at run-time or otherwise. The outcome of such a
mismatch would simply be the undefined runtime behaviour. Section
\ref{sec:api_challanges} fleshes out how in detail how the APIs used to write
programs for GPUs work, and demonstrates other examples of how errors such as
these can occur.

It is worth noting that the problems described here are further complicated by
the fact that there are competing graphics API ``standards'', which can be
either ``open'' \cite{OpenGL} \cite{Vulkan} or locked to a single platform
\cite{Direct3D} \cite{Metal}. There can be different versions of the same
standard on the same platform \cite{OpenGLHistory}. There can be different
implementations of the same standards for different hardware on the same
platform \cite{NVIDIADrivers} \cite{NVIDIADrivers}. There can be different
implementations of the same standards for the \textit{same} hardware on
different platforms \cite{OpenGLGettingStarted}. Platform and hardware
variations can support different features and extensions, both within the
``open'' ecosystems \cite{VulkanExtensions} or using proprietary mechanisms
\cite{PhysXSDK} \cite{HairworksAMD}. Furthermore, developers can be expected to
use different toolchains depending on their use-cases
\cite{KhronosDeveloperOverview}. Finally, GPU vendors will put game-specific
optimisations and workarounds in their drivers so that those specific games
will perform well on their graphics cards \cite{WhyGamesAreWorseOnLinux}.
Therefore, games using standard APIs incorrectly, may work fine on GPUs from
one vendor but not on GPUs from the other \cite{TODO}. Section
\ref{sec:api_options} gives an extensive breakdown of what is summarised here.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\linewidth]{waveparticles_example}
\caption{The output of wave particles system as demonstrated in the paper.}
\label{fig:waveparticles_example}
\end{figure}

There has been research into pushing forward the state-of-the-art in developing
code for heterogeneous platforms in more programmer-friendly ways. However, as
covered in Section \ref{sec:related_work}, much of the research has either been
domain-specific or provided abstractions which have an unacceptable performance
overhead for most use-cases involving GPUs. For example, a common approach is
to design a unified language which can be compiled to a run-time that supports
GPUs, as is the case with Lime \cite{Lime2010}. However, by hiding the details
of the heterogeneous backends the language targets, there is an unavoidable
run-time overhead. Working with current toolchains is an odd mix of using an
API and writing code in a GPU-specific language -- research has primarily
focused on hiding the API and simplifying GPU-specific elements by designing
unified languages abstract those details. However, alternative approaches which
improve aspects of current toolchains do exist.

The section provided the motivation for the solutions described in this paper,
first by demonstrating the prevalence of GPUs and how important improvements to
toolchains which target them could be. Secondly, it demonstrated the problems
current toolchains face with a brief example showing how data structures must be
defined separately in programming languages for the GPU and the CPU. Finally, a
brief explanation of why research has yet to tackle these problems was
provided.

\section{The Solutions}

\label{sec:solutions_introduction}

This section introduces two solutions to the problems described above. First by
describing what the desired outcomes of those solutions should be and then how
they should improve over the status quo. This is followed by the solutions;
first describing the annotation processor and then the pair of languages with a
unified type system. The pros and cons of each are then discussed to round off
the section.

\subsection{Desired Outcome}

As described in Section \ref{sec:related_work}, systems which aim to improve
over current toolchains often the sacrifice the control that traditional
workflows provide in order to improve the user-experience that programmers
have. The issue with this approach is the fact that performance is still
important in many domains where GPUs are used -- to the point where programmers
still choose to make the trade-off of working directly with APIs like OpenGL or
Vulkan instead of choosing the usability that abstractions could otherwise
provide \cite{TODO}. Therefore, solutions which aim to build and improve upon
the status quo should provide all the options that traditional toolchains do,
including exposing any underlying graphics or compute APIs.

Although this means these solutions are limited in the abstractions that they
can provide, there are some ``easy wins'' to be made. This is demonstrated by
the example problem shown in Listing \ref{lst:c_sharp_hlsl_struct_comparison}
and elaborated in Section \ref{sec:api_challanges}. Bugs may easily be
introduced to due errors in using teh API; these can prevented by compile-time
checks. That is, instead of trying to unify code that is written for the CPU
and GPU under a single language or system which is designed to abstract-away
API calls (thereby reducing the possibility of introducing errors as the
compiler generates API calls for the programmer), we can acknowledge that they
are different and so have host code and shaders written in different
programming languages. However, we can use compile-time type-checking
\textit{between} these languages and compile-time checks on the API calls
\textit{themselves} in order to have the compiler catch errors which would
otherwise be left undetected.

In conclusion, any solution that aims to improve upon current programming
models should deliver the benefits described above without sacrificing anything
current systems toolchains. This can be done by targetting specific classes of
errors as described in Section \ref{sec:api_challanges}. We can do this with
two techniques: compile-time cross-language checks and the processing of
annotations to check the validity of raw API calls.

\subsection{What was done}

We created two systems for type-checking programs across language boundaries in
CPU-GPU systems. Each one of these systems is focused on demonstrating a
different approach to implementing compile-time cross-language checks and
checking the validity of raw API calls. We chose to target the Vulkan API to
demonstrate that these ideas can work at a low-level. However, they are also
applicable to other APIs such as OpenGL, OpenCL or Direct3D.

The first system is a pre-processor for C and GLSL that operates on annotated
sections of code. The goal here was to demonstrate how an existing pair of
languages with similar type systems could be extended in order to have
error-checking on raw API calls.

The second is a proof-of-concept pair of languages with the features provided
by the pre-processing system baked-into the type system at the language level.
We had two goals with this. The first was to demonstrate that such a system
could work. The second was to show how existing high-level languages could be
extended with a shader-language. This would be suitable for a language such as
Rust, which has a strong type system, but is compromised and arguably
\textit{less safe} than C when using shaders. This is because C and GLSL can
share code as they have similar syntax. Rust and GLSL do not have similar
syntax, meaning that the problem shown in Listing
\ref{lst:c_sharp_hlsl_struct_comparison} becomes a problem.

\subsubsection{Annotation System}

The annotation system is a command line program that takes annotated code
written in C and annotated code written in GLSL as input. An example annotation
and the code that is generated is shown in Listings
\ref{lst:annotation_example_input} and \ref{lst:annotation_example_output}.

\begin{lstfloat}
\begin{lstlisting}[language=C]
VkComputePipelineCreateInfo compute_pipeline_create_info = {
    .sType = VK_STRUCTURE_TYPE_COMPUTE_PIPELINE_CREATE_INFO,
    .pNext = 0,
    .flags = 0,
    .stage = @create_verification(
        struct: VkComputePipelineCreateInfo,
        name: "test_shader",
        data: {
            stage: VK_SHADER_STAGE_COMPUTE_BIT,
            module: shader_module,
            pName: "main"
        }
    ),
    .layout = pipeline_layout,
    .basePipelineHandle = 0,
    .basePipelineIndex = 0
};
VkPipeline pipeline;
vkCreateComputePipelines(
    vulkan_device, 0, 1, &compute_pipeline_create_info,
    0, &pipeline
);
\end{lstlisting}
\begin{lstlisting}[language=C]
@verify(from: "test_shader", function:
void main() {
    output_buffer.values[gl_GlobalInvocationID.x] = input_buffer.values[gl_GlobalInvocationID.x];
})
\end{lstlisting}
\caption{Annotated C and GLSL, the output for these snippets (without
annotations) is shown in Listing \ref{lst:annotation_example_output}.}
\label{lst:annotation_example_input}
\end{lstfloat}

Listing \ref{lst:annotation_example_input} shows an example of an API call that
can result in an error that could be checked-for at compile-time. In this case
the running of a shader named \texttt{main}. Here, the host code makes the
\texttt{vkCreateComputePipelines} API call in order to create a compute
pipeline\footnote{This is just an API call that needs to be made TODO:
reference where more info on this can be found}
\cite{vkCreateComputePipelines}. An instance of the
\texttt{VkComputePipelineCreateInfo} containing a
\texttt{VkPipelineShaderStageCreateInfo} structure is passed to that API call,
which ultimately specifies which function in the shader to use within the
compute pipeline \cite{VkComputePipelineCreateInfo}
\cite{VkPipelineShaderStageCreateInfo}. Annotations are used to label the
creation of the \texttt{VkPipelineShaderStageCreateInfo} structure on the host
and the \texttt{main} function within the shader using the
\texttt{@create\_verification} and \texttt{@verify} commands respectively. The
annotation itself is given a name, \texttt{"test\_shader"} which both the host
code and shader reference. The annotation processor takes annotated code as
input, and produces output similar to Listing
\ref{lst:annotation_example_output}.

\begin{lstfloat}
\begin{lstlisting}[language=C]
VkComputePipelineCreateInfo compute_pipeline_create_info = {
    .sType = VK_STRUCTURE_TYPE_COMPUTE_PIPELINE_CREATE_INFO,
    .pNext = 0,
    .flags = 0,
    .stage = {
        .sType = VK_STRUCTURE_TYPE_PIPELINE_SHADER_STAGE_CREATE_INFO,
        .pNext = 0,
        .flags = 0,
        .stage = VK_SHADER_STAGE_COMPUTE_BIT,
        .module = shader_module,
        .pName = "main",
        .pSpecializationInfo = 0
    },
    .layout = pipeline_layout,
    .basePipelineHandle = 0,
    .basePipelineIndex = 0
};
VkPipeline pipeline;
vkCreateComputePipelines(
    vulkan_device, 0, 1, &compute_pipeline_create_info,
    0, &pipeline
);
\end{lstlisting}
\begin{lstlisting}[language=C]
void main()
{
    output_buffer.values[gl_GlobalInvocationID.x] = input_buffer.values[gl_GlobalInvocationID.x];
}
\end{lstlisting}
\caption{The output generated by Listing \ref{lst:annotation_example_input}.}
\label{lst:annotation_example_output}
\end{lstfloat}

The annotation provides two key features in this particular example. The first
is the generation of boiler-plate for the creation of the
\texttt{VkPipelineShaderStageCreateInfo} struct. This was done to investigate
boiler-plate generation features a pre-processor could provide. The second and
most important key feature is the compile-time checking of the function name --
ensuring that \texttt{main} cannot be renamed in the host or shader without
being consistently renamed in the other. If we were to ensure such an error was
introduced in raw C and GLSL, the resulting behaviour would be a runtime error.
However, not all inconsistencies which can be checked like this result in
runtime errors, others can simply result in incorrect or undefined runtime
results!

This example demonstrates how potential runtime errors or incorrect runtime
behaviour can be detected through the usage of annotations which are checked at
compile-time using a pre-processor. The flexibility of this system is that the
programmer can remove the annotations if they desire, especially if they are
using complex runtime features. However, this system can still be used to help
ensure that an initial implementation is correct. Furthermore, Section
\ref{sec:design_annotation_processor} covers design decisions that were made
with typical shader workflows in mind such that annotations can be kept for
most use-cases.

\subsubsection{New Language}

TODO:

\subsection{Pros and Cons}

TODO:


\section{Overview}

% This is the introduction where you should introduce your work.  In
% general the thing to aim for here is to describe a little bit of the
% context for your work -- why did you do it (motivation), what was the
% hoped-for outcome (aims) -- as well as trying to give a brief
% overview of what you actually did.

% It's often useful to bring forward some ``highlights'' into
% this chapter (e.g.\ some particularly compelling results, or
% a particularly interesting finding).

% It's also traditional to give an outline of the rest of the
% document, although without care this can appear formulaic
% and tedious. Your call.

TODO: Give overview of paper.

\section{Summary}

TODO: summarise introduction.

\chapter{Technical Background}

\label{chp:technical_background}

% A more extensive coverage of what's required to understand your
% work. In general you should assume the reader has a good undergraduate
% degree in computer science, but is not necessarily an expert in
% the particular area you've been working on. Hence this chapter
% may need to summarize some ``text book'' material.

% This is not something you'd normally require in an academic paper,
% and it may not be appropriate for your particular circumstances.
% Indeed, in some cases it's possible to cover all of the ``background''
% material either in the introduction or at appropriate places in
% the rest of the dissertation.

% This chapter covers relevant (and typically, recent) research
% which you build upon (or improve upon). There are two complementary
% goals for this chapter:
% \begin{enumerate}
%   \item to show that you know and understand the state of the art; and
%   \item to put your work in context
% \end{enumerate}

% Ideally you can tackle both together by providing a critique of
% related work, and describing what is insufficient (and how you do
% better!)

% The related work chapter should usually come either near the front or
% near the back of the dissertation. The advantage of the former is that
% you get to build the argument for why your work is important before
% presenting your solution(s) in later chapters; the advantage of the
% latter is that don't have to forward reference to your solution too
% much. The correct choice will depend on what you're writing up, and
% your own personal preference.

We introduce what a GPU is and how it is a good benchark for heterogeneous
hardware in general. We then provide extensive coverage of the relevant history
of graphics hardware and how it developed over time to become useful in many
fields beyond real-time rendering in video games. Following that, a breakdown
of graphics APIs is provided, to give both the context for the problem this
paper aims to tackle and the underlying technology the solutions depend upon.
Then, the difficulties and issues with programming for GPUs is summarised from
the contents of this chapter with a brief explanation of which ones the
solutions presented in this paper aim to tackle and which ones they do not. The
chapter finishes of with an overview of related research in this area,
demonstrating both were they succeed and how they fail to tackle the specific
problems that are solved here.

% TODO: Why low level details are important (eg unified memory versus seperate
% memory)
% https://www.slideshare.net/zlatan4177/gpgpu-algorithms-in-games
% https://en.wikipedia.org/wiki/Heterogeneous_System_Architecture

\section{The GPU}

\label{sec:gpu_hardware}

TODO:

\begin{center}
\begin{tabular}{||c||c|c|c||}
\hline
        & Number of Cores  & FLOPS                                         & Type of parallelism \\
\hline
\hline
CPU     & 1 - O(10)        & Up to 1 Teraflop \cite{IntelTeraFlop}         & TODO                \\
\hline
GPU     & O(10) - O(10000) & Up to 100 Teraflops \cite{NVIDIA100TeraFlops} & TODO                \\
\hline
\end{tabular}
\end{center}

% Can be worth being wary of GPU vs CPU performance based on performance alone.
% http://people.eecs.berkeley.edu/~sangjin/2013/02/12/CPU-GPU-comparison.html


\section{A (brief) History of GPU computing}

\label{sec:history_gpu}

Graphics Processing Units (GPUs) were originally fixed-function hardware
accelerators for 3D rendering aimed at hobbyist gamers who desired hardware to
play games with the most complex graphics possible. They really took off after
GLQuake demonstrated the improvements dedicated 3D graphics hardware could
achieve relative to CPUs \cite{GLQuake}. Since then, due to rapid developments
and competition, the scope and capabilities of graphics cards has expanded such
that they are the highly-parallel general computation machines we find them to
be today. However, the developments GPUs experienced weren't pre-planned, and
despite their current general-purpose nature, the terminology surrounding
graphics cards has its foundation in graphics. Furthermore, as the industry has
been so volatile, standards around using GPUs and heterogeneous hardware are
still solidifying, with many fragmented communities and confusion around the
future roadmap of both graphics and compute APIs.

This section summarises relevant milestones related to the history of
heterogeneous computing in order to help the reader understand why the field is
in its present state. However, it is not comprehensive and excludes many
details.

% \caption{
% Tabulated history of graphics APIs and graphics card development. Naturally
% this tabled misses many details, especially related to the earlier years when
% there were many hardware vendors, the graphics market was volatile, and there
% were many, many competing graphics APIs each with their own extensions and
% features. Futhermore, the introduction of certain features are really hard to
% pin-down. For example, there is no set-date where GPUs went from being
% fixed-function hardware to becoming programmable hardware. Changes such as
% these happened gradually, with extra configurability being slowly introduced to
% graphics hardware with extensions and additional API calls until shading
% languages needed to be defined in order to program these configurations more
% systematically. However, even these languages were limited in their
% capabilities and support for advanced features like loops and flow-control took
% time to be introduced into hardware.
% }
% \end{table}

\begin{itemize}

    \item 1992 \textbf{OpenGL 1.0} released by Silicon Graphics (SGI) as an API
    for interacting with graphics hardware on 3D graphics workstations. OpenGL
    was created from their proprietary ``IrisGL'' API in order to prevent
    competitors from creating their own proprietary graphics APIs and
    fragmenting the market. The API allowed users to issue basic commands in
    order to set-up 3D scenes, apply basic fixed-function lighting, and then
    render the scenes \cite{OpenGL_1_0}. Control of the OpenGL specification
    eventually transfers from SGI to various owners before landing in the hands
    of the \textbf{Khronos Group} \cite{OpenGL}.

    \item 1996 \textbf{Direct3D 2.0} release by Microsoft, so named, because it
    was the \textit{first} version of Direct3D. Although at this point API was
    designed as an abstraction for a software renderer
    \cite{JohnCarmackPlanDirect3DvsOpenGl}, it eventually developed into an API
    for interfacing with graphics cards.

    \item 1996 \textbf{3Dfx Voodoo1} graphics card released. Although not the
    first graphics card, it dominated the consumer market and brought 3D
    graphics rendering to the mainstream \cite{Voodoo1}.

    \item 1999 \textbf{NVIDIA GeForce 256} released; the first video card to be
    called a \textbf{GPU}. It was the first consumer-PC graphics hardware to
    accelerate ``transform and lighting'' operations, allowing them to
    offloaded form the CPU. The ``transform and lighting'' component of GPUs
    prefigured what would eventually run shaders \cite{GeForce256}.

    \item 2000 \textbf{The Khronos Group} founded to provide structure for open
    standards in 3D graphics. Reflecting the increasing flexibility of graphics
    cards over time, they now also provide standards virtual and augmented
    reality, heterogeneous computing, computer vision, parallel computing and
    neural networks \cite{KhronosGroupAbout}. In 2006 control of OpenGL was
    transferred to the Khronos Group \cite{OpenGLToKhronos}.

    \item 2001 \textbf{GPGPU}, \textit{general purpose computations on GPUs},
    are demonstrated by Larson and McAllister. They perform fast matrix
    multiplications by encoding the matrices as the inputs and outputs of the
    graphics pipeline such that the graphics pipeline can then be configured to
    perform the desired calculations \cite{MatrixGPU}. Subsequent papers then
    show how GPUs can outperform CPUs for many operations that are desirable in
    scientific computing \cite{CUDAtoOpenCL} \cite{Kruger03linearalgebra}
    \cite{LUGPU} \cite{SparsematrixGPU}.

    \item 2003 \textbf{OpenGL ES}, \textit{OpenGL for embedded systems},
    released by the Khronos Group \cite{OpenGLESRelease}. It is currently the
    most widely deployed 3D graphics API in history, being popular on mobile
    platforms such as Google's Android operating system \cite{OpenGLES}.

    \item 2004 \textbf{GLSL} (also known as OpenGL Shading Language or glslang)
    introduced into the \textbf{OpenGL 2.0} specification. It was created to
    tame the complexity and proliferation of custom OpenGL extensions with
    assembly languages that customised parts of the fixed-function pipeline.
    This formally replaced parts of the graphics pipeline with
    user-programmable stages which were defined using a hardware-independent
    high-level language (replacing proprietary assembler languages that
    individual GPU manufacturers were introducing). GLSL at this point was
    defined as \textit{two} closely-related shading languages for different
    parts of the pipeline: the vertex processor (for modifying vertices) and
    the fragment processor (for modifying the final image). Finally, the
    language was forward-looking in supporting many features that hardware at
    the time did not \cite{GLSL_1_10}.

    \item 2005 \textbf{Unified Shader Architecture} introduced in ATI's
    \textbf{Xenos GPU}, released as part of the Xbox 360. The ``vertex'' and
    ``pixel'' segments of the GPU pipeline are unified into a single component
    that can run either. This brought GPUs closer to being highly-parallel
    general computation machines and away from being direct fixed-function
    hardware implementations of the graphics pipeline represented by APIs such
    as OpenGL \cite{XenosDemystified}. All GPUs would soon adopt this
    architecture \cite{HistoryOfTheGPU}.

    \item 2006 \textbf{CUDA} (Compute Unified Device Architecture) introduced
    by NVIDIA as a parallel computation platform and API. This allowed
    programmers to exploit the \textbf{GPGPU} capabilities of graphics hardware
    directly, without having to encode data as the input and output of the
    OpenGL pipeline. This made GPU computing more efficient and accessible
    \cite{AboutCUDA}.

    \item 2008 \textbf{Open CL} platform developed and announced by Apple as a
    potential open standard for GPGPU computing. Existing as a distinct entity
    from OpenGL, it has its own shader language called \textbf{OpenCL C}. Apple
    handed control of the OpenCL specification to the Khronos Group, with the
    first implementation of the standard releasing alongside the Snow Leopard
    OS X update in 2009 \cite{OpenCL}. OpenCL also added support other
    heterogeneous architectures such as DSPs and FPGAs.

    \item 2015 \textbf{SPIR-V} shading language released alongside
    \textbf{OpenCL 2.1} \cite{SPIRVLaunch}. Unlike the high-level
    \textit{OpenCL C} and \textit{GLSL} shading languages which drivers need to
    compile themselves, this is an intermediate language based on \textit{LLVM
    IR} which compilers for any language can target. \cite{LLVMIR}
    \cite{SPIRV}. SPIR-V support was then added to \textbf{OpenGL 4.6} and
    supported by Vulkan at launch, unifying the shading langauges of Khronos'
    APIs \cite{SPIRVOpenGL}.

    \item 2016 \textbf{Vulkan} released by the Khronos Group. A low level
    graphics and compute API for graphics cards that aims to better represent
    how modern graphics cards are designed, this differs from OpenGL, whose
    abstraction of a traditional graphics pipeline no longer represents the
    unified nature of GPUs \cite{VulkanAnnouncement}. Unlike OpenGL and OpenGL
    ES, Vulkan is the same on both desktop, mobile and embedded systems
    \cite{Vulkan}.

\end{itemize}


The capabilities of GPUs and their related APIs have massively expanded since
they were both introduced. This has allowed GPUs to be used in increasingly
general domains, including scientific computing, crypto-currency mining,
high-frequency trading, and artificial intelligence \cite{GPUCrypto}
\cite{GPUScientificComputing} \cite{GPUTrading} \cite{GPUAI}. Some hobbyists
for home the cards were originally designed are even being priced out of the
market \cite{GPUCrypto} \cite{BitcoinRuiningPricing}. However, there are many
different standards and APIs for different use-cases of the same hardware as a
result of a complex history. Furthermore, many of these use-cases have
widely-used proprietary alternatives such as CUDA, DirectX, Metal and even
proprietary game console APIs \cite{CUDA} \cite{Metal} \cite{Direct3D}
\cite{PS4PortCrew}. However, with the release of SPIR-V, aspects of the various
Khronos Standards are converging. Additionally, despite a foundation in
graphics, the compute capabilities of Vulkan are expanding such that OpenCL
could eventually be merged into the API \cite{VulkanOpenCLMerge}. Furthermore,
the low-level nature of new graphics and compute APIs such a Vulkan, Metal and
DirectX 12 have allowed portability initiatives to bring implementations of
those APIs as light shims above the others \cite{VulkanPortabilityInitiative}
\cite{VulkanPortabilityInitiativeAnnouncement}.

\section{The Challenges with the APIs}

\label{sec:api_challanges}

NOTE: THIS SECTION IS CURRENTLY UNSTRUCTURED, PLEASE FEEL FREE TO COMMENT ON
THE CONTENT, HOWEVER, DUE TO VARIOUS REWRITES AND EDITS OF THIS SECTION AND
OTHER PARTS OF THE DOCUMENT, I NEED TO RESTRUCTURE AND ENSURE IT FITS WITHIN
THE OVERALL FLOW.

% TODO mention how Vulkan is merging the OpenGL, OpenCL and OpenGL ES.

% TODO: Good comparison of mess vs potential future (Unity Pipeline).
% http://www.g-truc.net/doc/2015%20-%20EuroLLVM%20-%20SPIR-V.pdf
% can also be shown in conclusion.

This section gives an overview of the general challenges programmers face when
using graphics APIs. Firstly, the context and history of graphics APIs are
described, with a focus on OpenGL and Vulkan. Subsequently, the general
workflow of writing programs for GPUs is described, before finally the
individual problems that type heterogeneous type safety seeks to address are
identified.

Programmers interact with GPUs using APIs, however, as the development of and
growth of GPUs in computing has been relatively organic and ad-hoc, these APIs
themselves have many legacy components and can be difficult to work with
\cite{NVIDIAInternshipLessons}.

% TODO: can reference this blog:
% http://aras-p.info/blog/2015/03/13/thoughts-on-explicit-graphics-apis/
% http://aras-p.info/blog/2014/05/31/rant-about-rants-about-opengl/

To understand the challenges of working GPUs, one must first both understand
the current workflow of GPUs and how that workflow has developed over time.

The best point of comparison is the CPU, and programming for the CPU. When
programming for the CPU, programmers often have access to a plethora of
programming languages which are suited to different needs and are capable of
targeting a wide range of backends. Furthermore, the instruction sets of CPUs
are published and well-documented, allowing programmers to write applications
for them directly -- even allowing them to write their own compiler for them
if they so desire. Finally, in the world of desktop computing, x86 is the
de-facto standard architecture, massively increasing the portability of
programs which target desktop PCs without the need for hardware abstractions
which could come with performance penalties.

The workflow of the GPU is very different.

\subsection{OpenGL}

Although GPUs do not have a standard ISA, open standards for GPU computing have
been developed and maintained by the Khronos Group \cite{TODO}, which was
founded in 2000 by a group of companies for this purpose. OpenGL is the most
widely used standard graphics API that they maintain, however, the API has a
history dating back to the early 1990's \cite{TODO}, with control of the
standard being passed from Silicon Graphics in 2006 \cite{TODO}. For a long
time the OpenGL family of graphics APIs were the only cross-platform family of
graphics APIs, with OpenGL ES, a related API, being used for mobile and
embedded systems \cite{TODO}.

Despite OpenGL being an open standard with a differently-flavoured API for
non-desktop systems, portability and development using the API is difficult for
several reasons. The focus of this is on the language-aspects of graphics
programming.

The largest competitor to OpenGL has been Direct3D by Microsoft, and is an
alternative graphics API for Windows computers \cite{TODO}. Other platforms may
have their own proprietary graphics APIs, such a Metal for Apple devices
\cite{TODO}, and the various APIs that gaming consoles provide \cite{TODO}.
Although each of these platforms have their differences, the problems being
tackled can be generalised to all of them. Therefore, this paper will purely
focus on the OpenGL family of graphics APIs.

\subsection{Vulkan}

TODO:

\subsection{Graphics workflow and pipeline}

GPUs are highly parallel compute machines capable of running programs called
\textit{shaders}. However, unlike CPU programs, where a programmer will simply
start writing code in a \texttt{main} function which can be straightforwardly
compiled and executed, shaders require more setup and boilerplate. Typically,
shaders complement programs designed for the CPU, with the traditional example
being a video game. In this case, the core components of a game (such as logic,
animation, scene-setup, input handling) will be handled by the CPU which can
then offload certain computations to the GPU (such as graphics and physics).
The programmer does this by using graphics APIs to initialise the graphics
pipeline to the desired state for a particular computation (setting up buffers,
loading data onto the GPU, initialising the pipeline, reserving resources,
etc.). Following this, those APIs are used to direct the drivers to load
\textit{shader code} onto the GPU. Finally, the shaders are executed for the
desired results.

Although laborious, this workflow allows programmers to use the resources of
GPUs fairly efficiently. However, it is not without issues. The one which this
paper seeks to tackle is down to the fact that \textit{shaders} and CPU code
are written in completely different programming languages.

\begin{itemize}

    \item standard has a lot of legacy and issues:

        \item different workflows and APIs for different applications

        \item non-standard implementations of the standard, with
        vendor-specific extensions and other proprietary technologies.

    \item standard changes depending on whether targeting desktop, mobile
    or embedded systems

    \item must interact with Operating systems directly, such as using
    their windowing system to create a \textit{context}. This reduces
    portability.

    \item Languages are compiled at run-time, with different shader-models

    \item Companies Will partner with different vendors

    \item Examples

    \item Portability

    \item Fuzzing \cite{GLFuzz}

    \item Drive white-lists \cite{NVIDIAInternshipLessons}

\end{itemize}

\subsection{API options}

\label{sec:api_options}

\begin{table}
\footnotesize
\begin{tabu} to 1
\textwidth {||X[c]||X[c]|X[c]|X[c]||}
\hline
Name &
Created &
Developer &
Purpose \\
\hline
OpenGL &
1992 &
Khronos Group &
Graphics \\
\hline
Direct3D &
1996 &
Microsoft &
Graphics \\
\hline
OpenGL ES &
2003 &
Khronos Group &
Graphics \\
\hline
CUDA &
2007 &
NVIDIA &
Compute \\
\hline
OpenCL &
2009 &
Khronos Group &
Compute \\
\hline
Metal &
2014 &
Apple &
Compute and Graphics \\
\hline
Vulkan &
2016 &
Khronos Group &
Compute and Graphics \\
\hline

\hline
\hline

\hline
Name &
Targets &
Langauge &
License \\
\hline
OpenGL &
Desktop and console GPUs &
GLSL and SPIR-V &
Open Specification \\
\hline
Direct3D &
GPUs with Windows OS &
HLSL &
Proprietary \\
\hline
OpenGL ES &
GPUs on mobile and embedded systems &
GLSL &
Open Specification \\
\hline
CUDA &
NVIDIA GPUs &
CUDA C/C++ &
Proprietary \\
\hline
OpenCL &
GPUs, DSPs and FPGAs &
OpenCL C and SPIR-V &
Open Specification \\
\hline
Metal &
GPUs with macOS or iOS &
Metal Shading Language (C++ based) &
Proprietary \\
\hline
Vulkan &
GPUs &
SPIR-V &
Open Specification \\
\hline
\end{tabu}

\caption{ A comparison of different graphics and compute APIs, listed in
chronological order. These represent lowest-level interfaces for interacting
with heterogeneous hardware. The fragmentation seen here has resulted in an
ecosystem very different from the ``hourglass model'' that CPUs enjoy. OpenCL,
OpenGL, OpenGL ES and Vulkan are simply four standards from a \textit{single
body} for interacting with the same underlying hardware. Furthermore, there are
many proprietary alternatives hardware and operating system vendors like to
push themselves.}

%TODO: add references to this table.

\label{tbl:api_comparison}

\end{table}

\subsection{Issues Summary}

TODO;

\begin{lstfloat}
\begin{lstlisting}[language=C]
const float fixedDeltaTime;
const int currentFrame;
const float particleSpeed;
float2 getPosition(int startingFrame, float2 velocity, float2 origin) {
    float t = (fixedDeltaTime * (float)(currentFrame - startingFrame));
    return origin + (t * particleSpeed * velocity);
}

const int horiRes;
const int vertRes;
const float planeWidth;
const float planeHeight;
StructuredBuffer<WaveParticle> waveParticleBuffer;
RWTexture2D<float4> amplitudeTexture;

///
/// Splat the wave particles to the splatTexture
///
#pragma kernel SplatParticles
[numthreads(THREAD_GROUPS_X, 1, 1)]
void DrawParticles(uint3 id : SV_DispatchThreadID)
{
    WaveParticle particle = waveParticleBuffer[id.x];
    float2 waveParticlePosition = getPosition(
        particle.startingFrame, particle.velocity, particle.origin
    );
    int xPos = (int)round((waveParticlePosition.x / planeWidth) * horiRes);
    int yPos = (int)round((waveParticlePosition.y / planeHeight) * vertRes);
    amplitudeTexture[int2(xPos, yPos)] +=
        float4(0.0, particle.amplitude, 0.0, 0.0);
}
\end{lstlisting}
\label{lst:draw_particles}
\caption{An example of an HLSL computer shader that takes a particle stored in
in a buffer and copies its amplitude to a specific location on a texture.}
\end{lstfloat}

\section{Related Work}

\label{sec:related_work}

This section describes related work in the field of modifying or improving GPU
toolchains beyond traditional low-level libraries. It aims to show the diverse
set of approaches that have been taken to doing this and the progress that has
been made, in addition to demonstrating that the specific problems targeted by
this paper have yet to be addressed.

Since the creation of the SPIR-V intermediate language for heterogeneous
programming (as discussed in Chapter \ref{chp:technical_background}), some
progress has been made in simplifying and unifying how GPUs are interacted with
for compute purposes. For example the Khronos Group have recently standardised
a modified version of C++, \textit{OpenCL C++} for developing compute kernels
\cite{OpenCL22Release} \cite{OpenCLCPPWhitePaper} \cite{OpenCL}. This has only
been possible because it can be compiled to SPIR-V, whereas previously graphics
drivers had to implement OpenCL C directly, limiting how complex OpenCL's
shader language could become. However, despite unifying some aspects of host
and accelerator languages, this uses the traditional workflow of programming
compute kernels and host code separately, with no cross-module checking at the
API boundaries \cite{OpenCL22Release}.

SYCL is a framework by the Khronos Group built on top of OpenCL that abstracts
away the API calls to enable ``single-source'' C++ development which
automatically generates host and kernel code from a single C++ source file
\cite{OpenCL22Release} \cite{SYCL}. SYCL has seen uptake in terms of multiple
implementations \cite{ComputeCPP} \cite{triSYCL}, and has been adopted as a
backend for various machine learning frameworks such as TensorFlow and Eigen
\cite{SYCLTensorFlow} \cite{SYCLEigen}. However, this approach does have its
limitations in removing the ability for developers to control how those API
calls are made. Both OpenCL and SYCL are also limited in only being suitable
for \textit{compute} applications, and do not allow for the graphics operations
that GPUs are capable of. However, Vulkan and OpenCL could merge in the future,
which may open the door to this happening \cite{VulkanOpenCLMerge}.

Research has been done to make GPU and heterogeneous computing available in
high-level dynamic languages. Theano and TensorFlow do this by abstracting a
compute platform such as OpenCL or CUDA using a framework or library
\cite{Theano2016} \cite{TensorFlowWhitePaper}. This technique has primarily
seen success in scientific computing and machine learning. Others have created
custom high-level languages which target GPUs directly. Harlan, developed by
Holk et al., is a domain-specific langauge (DSL) based on Scheme for GPUS, with
support for higher order programming with Scheme-influenced syntax and
semantics \cite{Harlan} \cite{HarlanAnnouncement}. GPipe extends the functional
language Haskell such that shaders can be written functionally and OpenGL calls
can be made in type-safe ways, however, the garbage-collected nature of Haskell
makes it unsuitable for many of the domains GPUs are used in such as games
\cite{HaskellState} \cite{GPipe}. Halide is a DSL for image processing and
computational photography \cite{Halide}. Fumero et al. have developed
techniques to automatically offload computation from high-level interpreted
dynamic languages using just-in-time compilation to achieving speedups by
compiling R to an OpenCL C backend \cite{JITGPU}. Futhark is an attempt to
design a functional data-parallel array language from the ground-up that can
target GPUs, targeting an OpenCL backend \cite{Futhark}.

Existing langauges have also been extended with the functionality to compile
parts of their programs to heterogeneous architectures. Lime and JCUDA take the
approach of compiling Java or Java-like code in such a manner \cite{Lime2010}
\cite{Lime2012} \cite{JCUDA2009}. However, they differ in their approaches.
Lime aims to abstract low-level details of heterogeneous computing such that
arbitrary backends can be targeted, including GPUs (using OpenCL as a back-end)
and FPGA synthesisation. JCUDA targets CUDA, allowing CUDA programs to be
created using Java, with an interface that aims to closely match CUDA's native
C API. OpenACC is a system specifically targeted at scientists that lets
programmers mark appropriate sections of normal C++, C or Fortran code as
possible candidates for accelerated parallel computation \cite{OpenACC}. The
goal is specifically to allow users to write code as they normally would for
the CPU, and then add directives to code fragments as hints to the compiler
that those fragments could perform optimally if run on a GPU.

Another approach has that has been taken is to abstract away underlying APIs
such as CUDA and OpenCL with platforms that can target both as backends. HIP is
a project that does this by allowing CUDA code to be compiled to C++ that uses
an abstracted API so that developers can convert their CUDA projects to
something that can target arbitrary backends \cite{HIP}.

In addition to SYCL and OpenCL C++, there have been other attempts at bringing
heterogeneous programming models natively to C++ through parallel programming
standards. C++ AMP is a C++ library, programming model, and compiler developed
by Microsoft that targets DirectX 11 \cite{CAMP}. However, it seems to have
died \cite{CAMPFail1} \cite{CAMPFail2}. \textit{C++ extensions for parallelism}
has brought such standards to the C++ standard template library
\cite{CPPParallelism}. Finally, OpenMP is an API standard dedicated to
shared-memory multiprocessing \cite{OpenMP}. HCC is a project aims to take C++
code that conforms to any of these standards and compile it to AMD's GCN
instruction set \cite{HCC}.

As can be seen, there has been progress in making programming for heterogeneous
architectures easier. However, the focus has been primarily on GPGPU computing.
Futhermore, although the creation of SPIR-V and Vulkan are slowly changing
this, all of these systems have had to target fairly-high level backend APIs
such as OpenCL or CUDA, in addition to languages interpreted by device drivers
such as \textit{OpenCL C} or \textit{CUDA C/C++}, which has limited the
performance and stability of the higher-level systems \cite{GLFuzz}.
Additionally, apart from \textit{Open CL C++}, these all either present
alternate high-level interfaces or abstract them away in order to ensure
simplified and less error-prone programs. This differs from the approach taken
here, which is to mark-up low-level API calls directly, so that they are
checked at compile-time for consistency without sacrificing the control that
would otherwise be lost by abstractions. Furthermore, we targeted both
\textit{compute} and \textit{graphics} back-ends, as opposed to focusing only
on compute.

% TODO: Future roadmap confusion

\chapter{Design}

NOTE: THIS CHAPTER IS CURRENTLY INCOMPLETE, AND WILL BE FELSHED-OUT BASED ON
FEEDBACK TO THE OTHER SECTIONS.

The project is composed of two systems. The first is a pre-processing system
for C and GLSL that ensures the enforcement of interfaces between the two
languages. The second is a set of two novel toy languages which natively
support such enforcement. How these are used is described in Section
\ref{sec:solutions_introduction}

\section{Design Goals}

The ultimate design goal of the cross-module type checking systems is to make
programming for GPUs less frustrating and error prone by catching errors which
can commonly occur. However, unlike other systems which do this, we cannot have
\textit{any} compromises to runtime performance relative to existing industry
standards such as GLSL and OpenGL. This includes not limiting the possible
programs programmers can make. Naturally there pros and cons to this approach.

There are other concerns beyond performance that need to be taken into account
which inform the design the system. For example, a commonly used feature is two
\textit{swap} shaders with identical interfaces in-and-out at runtime.
Therefore this needs to be supported by the system as well.

\begin{itemize}

    \item Having a non-intrusive system

    \item supporting simple workflows, but without sacrificing control

    \item Allow different shaders to be type-checked at compile-time but
    swapped in-and-out at run-time.

    \item Allow custom wrappers to exist.

\end{itemize}

\section{Workflow}

One of the important parts of the system is the implementation of the workflow.

\section{Annotation Processor}

\label{sec:design_annotation_processor}

TODO

\section{Language}

TODO

\chapter{Implementation}

% This chapter may be called something else\ldots but in general
% the idea is that you have one (or a few) ``meat'' chapters which
% describe the work you did in technical detail.

The source code for the project is publicly available and open-source
\cite{ProjectSource}. The implementation is formed of two distinct components,
the C-annotation pre-processor and the custom-languages compiler. Although both
are written in C++, Python is also used as a build-system to enable simple
cross-platform builds. The project was developed and tested on both Ubuntu
16.04 and Windows 10 operating systems.

\section{Pre-processor}

TODO:

\section{Compiler}

TODO: reference usage of FNV hash for symbol table. \cite{FNVHash}.


\chapter{Evaluation}

% For any practical projects, you should almost certainly have
% some kind of evaluation, and it's often useful to separate
% this out into its own chapter.


\section{Performance}

Ideally, there shouldn't be any performance overhead in either of the
solutions. For the annotation processor, that's fairly straight-forward to
establish, as it is essentially syntactic sugar that sits on-top-of C and GLSL.

However, although the custom language has semantics very similar to C and GLSL,
and in theory identical programs in both environments should be \textit{just
as} performant as C - the compiler may not optimise as well as it should.

TODO:


\chapter{Conclusion, Further Work and the Future of Heterogeneous Programming}

% As you might imagine: summarizes the dissertation, and draws
% any conclusions. Depending on the length of your work, and
% how well you write, you may not need a summary here.

% You will generally want to draw some conclusions, and point
% to potential future work. \cite{DirectXWorkings}

TODO:

NOTE: CURRENTLY ONLY CONTAINS SPECULATION ON THE FUTURE OF HETEROGENEOUS
PROGRAMMING. IT IS CURRENTLY SLIGHTLY UNSTRUCTURED BUT PRESENTS THE IDEAS
I THINK THE FINAL CONCLUSION WILL.

Something which the creation of SPIR-V enables, and that this paper
demonstrates is possible, is the ability to \textit{extend} langauges with
custom shading langauges whilst still preserving existing API interfaces. This
is different from that currently done both in industry and reasearch. For
example, whilst programs for the Unity game engine are written in C$^\sharp$,
shaders for it have to be written in HLSL \cite{TODO}. Projects such as SYCL
and LIME did extend C++ and Java respectively to allow shaders to be programmed
in their host languages, however they used optimising compilers with the goal
of abstracting away the underlying OpenCL API and outputting OpenCL C code for
shaders. However, OpenCL C++ has demonstrated that API interfaces can be
preserved whilst shaders are written in the host language
\cite{OpenCLCPPWhitePaper}. This also means that SYCL can use this compiler in
its backend \cite{TODO}.

This leads into the approach I expect other high-level languages to take in the
future as, briefly summarised below:

\begin{itemize}

    \item Port desired graphics and compute APIs such as OpenGL, OpenCL and
    Vulkan to your language so that they can be called directly from it.

    \item Create a modified extension of your language that can be used to
    program shaders, the compiler for this extension can target SPIR-V.

    \item Use any features your language may have to increase the safety of
    specific API calls using techniques similar to what has been demonstrated
    in this paper. Alternatively custom annotations may be used to strengthen
    this system.

    \item Create ``single source'' abstractions which an optimising compiler
    can use to generate appropriate API calls for your computations, taking
    advantage of your shader compiler. The relationship here can be similar to
    the one between SYCL and OpenCL C++.

\end{itemize}

The benefits of this approach is that the GPU ecosystem will more closely
resemble the ``hourglass model'' that CPUs enjoy today, instead of the separate
fragmented ecosystems it currently suffers. Eventually, the higher-level OpenCL
and OpenGL could be APIs which are implemented in terms of the lower-level
Vulkan, bringing this ``dream'' another step closer to reality
\cite{OpenGLonVulkan} \cite{VulkanOpenCLMerge} \cite{OpenGLOverload}.

\appendix
\singlespacing

\bibliographystyle{unsrt}
\bibliography{references}

\end{document}
